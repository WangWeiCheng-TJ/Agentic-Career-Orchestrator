import os
import glob
import chromadb
import google.generativeai as genai
from termcolor import cprint
from dotenv import load_dotenv
import json

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json
from src.utils import extract_text_from_pdf

load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
RAW_DATA_PATH = "/app/data/raw" # é€™è£¡æ”¾ä½ æ‰€æœ‰çš„å€‹äººè³‡æ–™ (PDF/MD/TXT)

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

def extract_text(file_path):
    """
    æ™ºæ…§è®€å–ï¼šå…ˆå˜—è©¦ä¸€èˆ¬è®€å–ï¼Œè®€ä¸åˆ°å°±åˆ‡æ› OCRã€‚
    """
    ext = os.path.splitext(file_path)[1].lower()
    filename = os.path.basename(file_path)

    try:
        # === è™•ç† PDF ===
        if ext == ".pdf":
            # ä½¿ç”¨ utils ä¸­çš„ extract_text_from_pdf (åŸºæ–¼ utils.py:12)
            text, used_ocr = extract_text_from_pdf(file_path, model_name=MODEL_NAME)
            # [ä¿®æ­£é» 1] å›å‚³é€šç”¨çš„ "pdf_document"ï¼Œä¸è¦åœ¨é€™è£¡å®šæ­»å®ƒæ˜¯ resume
            return text, "pdf_document"

        # === è™•ç†ç­†è¨˜ (MD/TXT) ===
        elif ext in [".md", ".txt"]:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read(), "personal_note"

        # === [NEW] è™•ç† JSON (user_profile.json) ===
        elif ext == ".json":
            with open(file_path, "r", encoding="utf-8") as f:
                json_content = json.load(f)
                
                # å¦‚æœæ˜¯ user_profile.jsonï¼Œæ¨™è¨˜ç‚ºç‰¹æ®Šé¡å‹ï¼Œä¸è¦éåº¦ summarize
                if filename == "user_profile.json":
                    text = json.dumps(json_content, indent=2, ensure_ascii=False)
                    return text, "user_profile"  # ç‰¹æ®Š doc_type
                else:
                    text = json.dumps(json_content, indent=2, ensure_ascii=False)
                    return text, "structured_data"

        else:
            return None, None

    except Exception as e:
        cprint(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}", "red")
        return None, None

def indexer_agent_process(filename, text, doc_type):
    # å¦‚æœæ˜¯ user_profileï¼Œç›´æ¥è·³é LLMï¼Œç”¨åŸå§‹ metadata
    if doc_type == "user_profile":
        return {
            "summary": "User Profile (Pre-computed cheat sheet)",
            "domain": "Career Profile",
            "tags": ["#UserProfile", "#Skills", "#Education"],
            "is_resume": False
        }
    else:
        prompt = f"""
        You are my Personal Data Archivist.
        I am ingesting a document into my personal knowledge base.
        
        Filename: {filename}
        Type: {doc_type}
        Content Snippet: {text}
        
        ### TASK
        1. Identify the **Topic/Domain** (e.g., "Resume V1", "Project Alpha Notes", "Research Idea").
        2. Extract **Keywords/Skills** mentioned.
        3. Summarize the content in one sentence.
        
        ### OUTPUT JSON
        {{
            "summary": "Brief summary of this file.",
            "domain": "Computer Vision / System Design / Career Profile",
            "tags": ["#Tag1", "#Tag2"],
            "is_resume": true/false
        }}
        """
        
        default_res = {
            "summary": "Processing Failed",
            "domain": "Unknown",
            "tags": [],
            "is_resume": False
        }

    return safe_generate_json(model, prompt, retries=3, default_output=default_res)

def generate_user_profile_from_raw():
    """
    [NEW] å¾ raw/ è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æª”æ¡ˆè‡ªå‹•ç”¢ç”Ÿ user_profile.json
    """
    cprint("ğŸ¤– è‡ªå‹•ç”¢ç”Ÿ user_profile.json...", "cyan")
    
    # è®€å–æ‰€æœ‰ raw æª”æ¡ˆå…§å®¹
    raw_content = ""
    for file_path in glob.glob(os.path.join(RAW_DATA_PATH, "*")):
        filename = os.path.basename(file_path)
        
        # è·³é user_profile.json æœ¬èº«
        if filename == "user_profile.json":
            continue
        
        content, doc_type = extract_text(file_path)
        if content:
            raw_content += f"\n\n=== {filename} ===\n{content}\n"
    
    if not raw_content:
        cprint("âŒ æ²’æœ‰å¯ç”¨çš„ raw æª”æ¡ˆä¾†ç”¢ç”Ÿ user_profile", "red")
        return None
    
    # ç”¨ LLM ç”¢ç”Ÿçµæ§‹åŒ– user_profile
    prompt = f"""
    You are extracting a structured user profile from personal documents.
    
    ### SOURCE DATA:
    {raw_content}
    
    ### TASK:
    Extract the following information into a structured JSON format:
    
    ### OUTPUT JSON SCHEMA:
    {{
      "name": "User's full name",
      "current_position": "Current job title",
      "education": [
        {{"degree": "PhD/Master/Bachelor", "field": "...", "institution": "...", "year": "..."}}
      ],
      "skills": ["Skill1", "Skill2", ...],
      "experience": [
        {{"role": "Job Title", "company": "...", "duration": "...", "highlights": ["..."]}}
      ],
      "research_interests": ["Topic1", "Topic2", ...],
      "languages": ["English", "Chinese", ...],
      "summary": "Brief professional summary in 2-3 sentences"
    }}
    
    Important: Extract ONLY information that is explicitly present in the documents. Use "Unknown" for missing fields.
    """
    
    default_profile = {
        "name": "Unknown",
        "current_position": "Unknown",
        "education": [],
        "skills": [],
        "experience": [],
        "summary": "Auto-generated profile from raw data"
    }
    
    generated_profile = safe_generate_json(model, prompt, retries=3, default_output=default_profile)
    
    # åŠ å…¥ metadata
    generated_profile["_metadata"] = {
        "source": "auto_generated",
        "generated_from": "data/raw/*",
        "note": "This is an automatically generated profile. For better results, manually create user_profile.json"
    }
    
    return generated_profile

def ingest_personal_data():
    cprint(f"ğŸš€ [Level 0] é–‹å§‹å»ºç½®å€‹äººçŸ¥è­˜åº« (Ingesting Personal Data)...", "cyan", attrs=['bold'])
    
    # === [NEW] Step 1: æª¢æŸ¥ä¸¦è™•ç† user_profile.json ===
    manual_profile_path = os.path.join(RAW_DATA_PATH, "user_profile.json")
    auto_profile_path = os.path.join(CHROMA_PATH, "auto_generated_user_profile.json")
    
    has_manual_profile = os.path.exists(manual_profile_path)
    
    if has_manual_profile:
        cprint("âœ… åµæ¸¬åˆ°æ‰‹å‹• user_profile.jsonï¼Œå°‡è·³éå…¶ ChromaDB ingestion", "green")
        cprint("   â†’ Phase 3 æœƒç›´æ¥è®€å–æ­¤æª”æ¡ˆï¼Œä¿ç•™å®Œæ•´çµæ§‹", "green")
    else:
        cprint("âš ï¸ æœªåµæ¸¬åˆ° user_profile.jsonï¼Œå•Ÿå‹•è‡ªå‹•ç”¢ç”Ÿæ¨¡å¼...", "yellow")
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ auto_generated ç‰ˆæœ¬
        if os.path.exists(auto_profile_path):
            cprint(f"â„¹ï¸  å·²å­˜åœ¨ auto_generated_user_profile.jsonï¼Œè·³éé‡æ–°ç”¢ç”Ÿ", "cyan")
        else:
            # ç”¢ç”Ÿæ–°çš„ auto_generated_user_profile.json
            generated_profile = generate_user_profile_from_raw()
            
            if generated_profile:
                os.makedirs(CHROMA_PATH, exist_ok=True)
                with open(auto_profile_path, 'w', encoding='utf-8') as f:
                    json.dump(generated_profile, f, indent=2, ensure_ascii=False)
                cprint(f"âœ… è‡ªå‹•ç”¢ç”Ÿå®Œæˆ: {auto_profile_path}", "green")
            else:
                cprint("âŒ è‡ªå‹•ç”¢ç”Ÿå¤±æ•—ï¼ŒPhase 3 å°‡åƒ…ä½¿ç”¨ ChromaDB æŸ¥è©¢", "red")
    
    # === Step 2: é–‹å§‹ ChromaDB Ingestion ===
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection = client.get_or_create_collection(name="personal_knowledge")
    
    files = glob.glob(os.path.join(RAW_DATA_PATH, "*"))
    
    count = 0
    skipped_count = 0
    
    for file_path in files:
        filename = os.path.basename(file_path)
        
        # 1. è®€å–
        content, doc_type = extract_text(file_path)
        if not content:
            continue
        
        # === [CRITICAL] è·³é user_profile.json çš„ ingestion ===
        if filename == "user_profile.json":
            cprint(f"\nâ­ï¸  è·³é {filename} (Phase 3 æœƒç›´æ¥è®€å–ï¼Œé¿å…è¢«å£“ç¸®)", "yellow")
            skipped_count += 1
            continue
        
        cprint(f"\nğŸ“„ åˆ†ææª”æ¡ˆ: {filename} ({doc_type})", "white")
        
        # 2. AI ç†è§£ & æ¨™è¨˜
        cprint("  ğŸ¤– Indexer Agent Analyzing...", "blue")
        metadata = indexer_agent_process(filename, content, doc_type)
        cprint(f"  ğŸ·ï¸  Domain: {metadata.get('domain')}", "green")
        cprint(f"  ğŸ“ Summary: {metadata.get('summary')}", "green")
        
        # 3. æ ¼å¼åŒ– Metadata
        storage_meta = {
            "filename": filename,
            "doc_type": doc_type,
            "domain": metadata.get("domain", "Unknown"),
            "tags": ", ".join(metadata.get("tags", [])),
            "is_resume": str(metadata.get("is_resume", False)),
            "summary": metadata.get("summary", "")
        }
        
        # 4. å­˜å…¥ ChromaDB
        try:
            collection.upsert(
                documents=[content],
                metadatas=[storage_meta],
                ids=[filename]
            )
            cprint("  âœ… Saved to Knowledge Base", "magenta")
            count += 1
        except Exception as e:
            cprint(f"âŒ DB Error: {e}", "red")
    
    cprint(f"\nğŸ‰ å»ºç½®å®Œæˆï¼ä½ çš„æ•¸ä½åˆ†èº«ç¾åœ¨æ“æœ‰ {count} ä»½è¨˜æ†¶ã€‚", "cyan", attrs=['bold'])
    if skipped_count > 0:
        cprint(f"   (è·³é {skipped_count} å€‹æª”æ¡ˆä»¥ä¿è­·çµæ§‹)", "yellow")

        
if __name__ == "__main__":
    ingest_personal_data()