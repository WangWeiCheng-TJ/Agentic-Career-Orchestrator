import os
import glob
import chromadb
import google.generativeai as genai
from termcolor import cprint
from dotenv import load_dotenv
from pypdf import PdfReader

# å¼•å…¥é˜²å‘†å·¥å…· (è«‹ç¢ºä¿ src/utils/llm_utils.py å­˜åœ¨)
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json
from src.utils import gemini_ocr

load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
RAW_DATA_PATH = "/app/data/raw" # é€™è£¡æ”¾ä½ æ‰€æœ‰çš„å€‹äººè³‡æ–™ (PDF/MD/TXT)

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

def extract_text(file_path):
    """
    æ™ºæ…§è®€å–ï¼šå…ˆå˜—è©¦ä¸€èˆ¬è®€å–ï¼Œè®€ä¸åˆ°å°±åˆ‡æ› OCRã€‚
    """
    ext = os.path.splitext(file_path)[1].lower()
    filename = os.path.basename(file_path)

    try:
        # === è™•ç† PDF ===
        if ext == ".pdf":
            text = ""
            try:
                reader = PdfReader(file_path)
                for page in reader.pages:
                    content = page.extract_text()
                    if content:
                        text += content + "\n"
            except Exception as e:
                cprint(f"   âš ï¸ pypdf è®€å–å¤±æ•—ï¼Œå˜—è©¦ OCR... ({e})", "yellow")
                text = "" # è¨­ç‚ºç©ºï¼Œè§¸ç™¼ä¸‹æ–¹çš„ OCR

            # [OCR é˜²å‘†é‚è¼¯]
            # å¦‚æœè®€å‡ºä¾†æ˜¯ç©ºçš„ï¼Œæˆ–æ˜¯å­—æ•¸å¤ªå°‘ (ä¾‹å¦‚ < 100 å­—ï¼Œå¯èƒ½åªæ˜¯åœ–è¡¨æˆ– header)ï¼Œå°±ç•¶ä½œå®ƒæ˜¯æƒææª”
            if not text or len(text.strip()) < 50:
                cprint(f"   ğŸ‘ï¸ åµæ¸¬åˆ°æƒææª”æˆ–ç´”åœ–ç‰‡ PDFï¼Œå•Ÿå‹• Gemini OCR: {filename}", "cyan")
                text = gemini_ocr(file_path, model_name=MODEL_NAME)
            
            # [ä¿®æ­£é» 1] å›å‚³é€šç”¨çš„ "pdf_document"ï¼Œä¸è¦åœ¨é€™è£¡å®šæ­»å®ƒæ˜¯ resume
            return text, "pdf_document"

        # === è™•ç†ç­†è¨˜ (MD/TXT) ===
        elif ext in [".md", ".txt"]:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read(), "personal_note"
        
        else:
            return None, None

    except Exception as e:
        cprint(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}", "red")
        return None, None

def indexer_agent_process(filename, text, doc_type):
    """
    ğŸ¤– Indexer Agent: ä¸ç®¡æ˜¯å±¥æ­·é‚„æ˜¯ç­†è¨˜ï¼Œé€šé€šè²¼ä¸Šæ¨™ç±¤
    """
    prompt = f"""
    You are my Personal Data Archivist.
    I am ingesting a document into my personal knowledge base.
    
    Filename: {filename}
    Type: {doc_type}
    Content Snippet: {text[:6000]}
    
    ### TASK
    1. Identify the **Topic/Domain** (e.g., "Resume V1", "Project Alpha Notes", "Research Idea").
    2. Extract **Keywords/Skills** mentioned.
    3. Summarize the content in one sentence.
    
    ### OUTPUT JSON
    {{
        "summary": "Brief summary of this file.",
        "domain": "Computer Vision / System Design / Career Profile",
        "tags": ["#Tag1", "#Tag2"],
        "is_resume": true/false
    }}
    """
    
    default_res = {
        "summary": "Processing Failed",
        "domain": "Unknown",
        "tags": [],
        "is_resume": False
    }

    return safe_generate_json(model, prompt, retries=3, default_output=default_res)

def ingest_personal_data():
    cprint(f"ğŸš€ [Level 0] é–‹å§‹å»ºç½®å€‹äººçŸ¥è­˜åº« (Ingesting Personal Data)...", "cyan", attrs=['bold'])
    
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    # æˆ‘å€‘æŠŠ Collection åå­—æ”¹å¾—æ›´é€šç”¨ä¸€é»ï¼Œå« "personal_knowledge"
    collection = client.get_or_create_collection(name="personal_knowledge")
    
    files = glob.glob(os.path.join(RAW_DATA_PATH, "*"))
    
    count = 0
    for file_path in files:
        filename = os.path.basename(file_path)
        
        # 1. è®€å–
        content, doc_type = extract_text(file_path)
        if not content: continue
        
        cprint(f"\nğŸ“„ åˆ†ææª”æ¡ˆ: {filename} ({doc_type})", "white")

        # 2. AI ç†è§£ & æ¨™è¨˜
        cprint("   ğŸ¤– Indexer Agent Analyzing...", "blue")
        metadata = indexer_agent_process(filename, content, doc_type)

        cprint(f"   ğŸ·ï¸ Domain: {metadata.get('domain')}", "green")
        cprint(f"   ğŸ“ Summary: {metadata.get('summary')}", "green")

        # 3. æ ¼å¼åŒ– Metadata
        storage_meta = {
            "filename": filename,
            "doc_type": doc_type,
            "domain": metadata.get("domain", "Unknown"),
            "tags": ", ".join(metadata.get("tags", [])),
            "is_resume": str(metadata.get("is_resume", False)), # Chroma ä¸å­˜ boolï¼Œè½‰å­—ä¸²
            "summary": metadata.get("summary", "")
        }

        # 4. å­˜å…¥ (æ•´ä»½å­˜å…¥ï¼Œä¸åˆ‡å¡Šï¼Œä¿æŒå®Œæ•´èªæ„)
        try:
            collection.upsert(
                documents=[content],
                metadatas=[storage_meta],
                ids=[filename]
            )
            cprint("   âœ… Saved to Knowledge Base", "magenta")
            count += 1
        except Exception as e:
            cprint(f"âŒ DB Error: {e}", "red")

    cprint(f"\nğŸ‰ å»ºç½®å®Œæˆï¼ä½ çš„æ•¸ä½åˆ†èº«ç¾åœ¨æ“æœ‰ {count} ä»½è¨˜æ†¶ã€‚", "cyan", attrs=['bold'])

if __name__ == "__main__":
    ingest_personal_data()