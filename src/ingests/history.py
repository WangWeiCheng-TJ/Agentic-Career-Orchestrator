import os
import glob
import time
import chromadb
import google.generativeai as genai
from termcolor import colored, cprint # æ”¹ç”¨ colored ä¾†ç”¢ç”Ÿå­—ä¸²ï¼Œäº¤çµ¦ tqdm å°
from dotenv import load_dotenv
from pypdf import PdfReader
from tqdm import tqdm # å¼•å…¥é€²åº¦æ¢

# === å¼•å…¥å·¥å…· ===
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json, gemini_ocr 

load_dotenv()
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")

# è·¯å¾‘è¨­å®š
PATH_ONGOING = "/app/data/history/ongoing"
PATH_REJECTED = "/app/data/history/rejected"

FORCE_UPDATE = os.getenv("FORCE_UPDATE", "False").lower() == "true"

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

def extract_text_smart(filepath):
    """æ™ºæ…§è®€å–ï¼šPDF -> Text, å¤±æ•—è½‰ OCR"""
    text = ""
    used_ocr = False
    
    try:
        reader = PdfReader(filepath)
        for page in reader.pages:
            content = page.extract_text()
            if content: text += content + "\n"
    except Exception:
        pass 

    # OCR åˆ¤æ–·
    if len(text.strip()) < 50:
        # é€™è£¡ä¸ print äº†ï¼Œäº¤çµ¦å¤–å±¤é¡¯ç¤ºç‹€æ…‹ï¼Œä¿æŒé€²åº¦æ¢ä¹¾æ·¨
        text = gemini_ocr(filepath, model_name=MODEL_NAME)
        used_ocr = True
    
    return text, used_ocr

def indexer_agent_history(filename, text, status):
    """ðŸ¤– History Indexer Agent"""
    prompt = f"""
    You are analyzing a PAST JOB APPLICATION (JD).
    Filename: {filename}
    Status: {status}
    Snippet: {text[:8000]}
    
    Extract JSON:
    {{
        "role": "Title",
        "experience_level": "Experience Level",
        "domain": "Domain",
        "tech_stack": ["Skill1", "Skill2"],
        "summary": "One liner",
        "tags": ["#Tag1"]
    }}
    """
    default = {"role": "Unknown", "experience_level": "Unknown", "domain": "Unknown", "tech_stack": [], "summary": "", "tags": []}
    return safe_generate_json(model, prompt, retries=3, default_output=default)

def process_folder(base_path, status_label, collection):
    search_path = os.path.join(base_path, "**", "*.pdf")
    files = glob.glob(search_path, recursive=True)
    
    if not files: return 0

    # é€™è£¡ç”¨ cprint æ²’é—œä¿‚ï¼Œå› ç‚ºé€²åº¦æ¢é‚„æ²’é–‹å§‹
    cprint(f"ðŸ“‚ æŽƒæç›®éŒ„: {base_path} ({len(files)} files)", "white")

    count = 0
    skipped_count = 0
    seen_ids = set() 

    # === [NEW] åˆå§‹åŒ–é€²åº¦æ¢ ===
    # desc: é€²åº¦æ¢å·¦é‚Šçš„æ–‡å­—
    # unit: å–®ä½
    pbar = tqdm(files, desc=f"Processing {status_label}", unit="file")

    for filepath in pbar:
        filename = os.path.basename(filepath)
        folder_name = os.path.basename(os.path.dirname(filepath))
        
        # å‹•æ…‹æ›´æ–°é€²åº¦æ¢å³é‚Šçš„è³‡è¨Š (é¡¯ç¤ºç•¶å‰æ­£åœ¨çœ‹å“ªå€‹æª”æ¡ˆ)
        pbar.set_postfix(file=filename[:20]) # åªé¡¯ç¤ºå‰20å­—å…ƒé¿å…å¤ªé•·

        # 1. è¨ˆç®— ID
        safe_status = status_label.replace("/", "_").replace(" ", "_")
        doc_id = f"history_{safe_status}_{folder_name}_{filename}"
        
        # 2. æª¢æŸ¥æ˜¯å¦å­˜åœ¨
        if not FORCE_UPDATE:
            existing = collection.get(ids=[doc_id])
            if existing and existing['ids']:
                skipped_count += 1
                continue # tqdm æœƒè‡ªå‹•æŽ¨é€²é€²åº¦æ¢ï¼Œä¸ç”¨æ‰‹å‹• update

        # --- é€²å…¥è™•ç†æµç¨‹ (æœƒèŠ±æ™‚é–“) ---
        
        # 3. è®€å–æ–‡å­—
        text, used_ocr = extract_text_smart(filepath)
        if not text or len(text) < 50:
            # ä½¿ç”¨ tqdm.write é¿å…æ‰“äº‚é€²åº¦æ¢
            tqdm.write(colored(f"   âš ï¸ [Skip] Empty content: {filename}", "yellow"))
            continue

        # 4. Agent åˆ†æž
        # åœ¨åš LLM é€™ç¨®è€—æ™‚æ“ä½œæ™‚ï¼Œå¯ä»¥æ›´æ–°ä¸€ä¸‹ description è®“ä½¿ç”¨è€…çŸ¥é“æ²’å¡æ­»
        pbar.set_description(f"ðŸ¤– AI Analyzing: {filename[:15]}...")
        
        meta = indexer_agent_history(filename, text, status_label)

        # 5. æº–å‚™ Metadata
        storage_meta = {
            "source": "history",
            "folder": folder_name,
            "filename": filename,
            "status": status_label,
            "role": meta.get("role", "Unknown"),
            "experience_level": meta.get("experience_level", "Unknown"),
            "domain": meta.get("domain", "Unknown"),
            "skills": ", ".join(meta.get("tech_stack", [])),
            "tags": ", ".join(meta.get("tags", [])),
            "summary": meta.get("summary", "")
        }

        # 6. å¯«å…¥ DB
        collection.upsert(
            documents=[text],
            metadatas=[storage_meta],
            ids=[doc_id]
        )
        
        # é¡¯ç¤ºæˆåŠŸè¨Šæ¯ (å°åœ¨é€²åº¦æ¢ä¸Šæ–¹)
        ocr_tag = colored(" [OCR]", "magenta") if used_ocr else ""
        msg = colored(f"   âœ… Indexed: {meta.get('role')} @ {folder_name}", "green")
        tqdm.write(msg + ocr_tag)
        
        count += 1
        
        # æ¢å¾©åŽŸæœ¬çš„ Description
        pbar.set_description(f"Processing {status_label}")
        
        if used_ocr: time.sleep(2)

    # è·‘å®Œè©²ç›®éŒ„å¾Œçš„ç¸½çµ
    if skipped_count > 0:
        tqdm.write(colored(f"   (Skipped {skipped_count} existing files)", "light_grey"))
        
    return count

def ingest_history_jds():
    cprint("\nðŸ“œ [Level 0] Building History Index (Incremental)...", "cyan", attrs=['bold'])
    
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection = client.get_or_create_collection(name="past_applications_jds")
    
    total_new = 0
    
    if os.path.exists(PATH_ONGOING):
        total_new += process_folder(PATH_ONGOING, "Ongoing", collection)
    
    print("-" * 40) # åˆ†éš”ç·š
    
    if os.path.exists(PATH_REJECTED):
        total_new += process_folder(PATH_REJECTED, "Rejected", collection)

    cprint(f"\nâœ… All Done! Added {total_new} new records.", "magenta", attrs=['bold'])

if __name__ == "__main__":
    ingest_history_jds()