import os
import glob
import chromadb
from pypdf import PdfReader
from termcolor import cprint
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
RAW_DATA_PATH = "/app/data/raw"

def extract_text_from_pdf(file_path):
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"âŒ ç„¡æ³•è®€å– PDF {file_path}: {e}")
        return None

def ingest_data():
    cprint(f"ğŸš€ é–‹å§‹è³‡æ–™æ³¨å…¥æµç¨‹...", "cyan")
    cprint(f"ğŸ“‚ æƒæç›®éŒ„: {RAW_DATA_PATH}", "cyan")

    # 1. é€£æ¥è³‡æ–™åº«
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ Chroma é è¨­çš„ Embedding æ¨¡å‹ (all-MiniLM-L6-v2)
    # å®ƒæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦åœ¨æœ¬åœ° CPU åŸ·è¡Œï¼Œå®Œå…¨å…è²»ä¸”éš±ç§ã€‚
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection = client.get_or_create_collection(name="job_experiences")

    # 2. æƒææª”æ¡ˆ
    files = glob.glob(os.path.join(RAW_DATA_PATH, "*"))
    documents = []
    metadatas = []
    ids = []

    for file_path in files:
        filename = os.path.basename(file_path)
        ext = os.path.splitext(filename)[1].lower()
        
        cprint(f"   ğŸ“„ è™•ç†æª”æ¡ˆ: {filename}", "white")
        
        content = ""
        doc_type = "unknown"

        if ext == ".pdf":
            content = extract_text_from_pdf(file_path)
            doc_type = "cv_or_paper"
        elif ext in [".txt", ".md"]:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            doc_type = "notes"
        else:
            print(f"   âš ï¸ è·³éä¸æ”¯æ´çš„æ ¼å¼: {filename}")
            continue

        if not content:
            continue

        # 3. ç°¡å–®åˆ‡åˆ† (Chunking)
        # ç‚ºäº† MVPï¼Œæˆ‘å€‘ç”¨ç°¡å–®çš„å­—å…ƒåˆ‡åˆ†ã€‚
        # é€²éšç‰ˆå¯ä»¥ç”¨ RecursiveCharacterTextSplitter (LangChain)
        chunk_size = 1000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

        for idx, chunk in enumerate(chunks):
            documents.append(chunk)
            metadatas.append({"source": filename, "type": doc_type, "chunk_index": idx})
            ids.append(f"{filename}_chunk_{idx}")

    # 4. å¯«å…¥è³‡æ–™åº«
    if documents:
        cprint(f"ğŸ’¾ æ­£åœ¨å¯«å…¥ {len(documents)} ç­†è³‡æ–™ç‰‡æ®µåˆ° ChromaDB...", "yellow")
        try:
            collection.upsert(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            cprint(f"âœ… è³‡æ–™æ³¨å…¥å®Œæˆï¼Collection ç¸½ç­†æ•¸: {collection.count()}", "green")
        except Exception as e:
            cprint(f"âŒ å¯«å…¥å¤±æ•—: {e}", "red")
    else:
        cprint("âš ï¸ æ²’æœ‰ç™¼ç¾æœ‰æ•ˆçš„æ–‡å­—è³‡æ–™ã€‚", "yellow")

if __name__ == "__main__":
    ingest_data()