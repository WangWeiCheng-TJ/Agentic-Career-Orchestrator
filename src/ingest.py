import os
import glob
import chromadb
from pypdf import PdfReader
from termcolor import cprint
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
RAW_DATA_PATH = "/app/data/raw"

def extract_text_from_pdf(file_path):
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"âŒ ç„¡æ³•è®€å– PDF {file_path}: {e}")
        return None

def ingest_data():
    cprint(f"ğŸš€ é–‹å§‹è³‡æ–™æ³¨å…¥æµç¨‹ (Recursive Splitter)...", "cyan")
    
    # åˆå§‹åŒ– ChromaDB
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection = client.get_or_create_collection(name="job_experiences")

    # åˆå§‹åŒ– LangChain åˆ‡åˆ†å™¨
    # é‚è¼¯ï¼šå„ªå…ˆåœ¨ \n\n (æ®µè½) åˆ‡ï¼Œä¸è¡Œæ‰åœ¨ \n (æ›è¡Œ) åˆ‡ï¼Œå†ä¸è¡Œæ‰åœ¨ç©ºæ ¼åˆ‡
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n", "\n", " ", ""]
    )

    files = glob.glob(os.path.join(RAW_DATA_PATH, "*"))
    documents = []
    metadatas = []
    ids = []

    for file_path in files:
        filename = os.path.basename(file_path)
        ext = os.path.splitext(filename)[1].lower()
        
        cprint(f"   ğŸ“„ è™•ç†æª”æ¡ˆ: {filename}", "white")
        content = ""
        doc_type = "unknown"

        if ext == ".pdf":
            content = extract_text_from_pdf(file_path)
            doc_type = "cv_or_paper"
        elif ext in [".txt", ".md"]:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            doc_type = "notes"
        else:
            continue

        if not content: continue

        # --- [å‡ç´š] ä½¿ç”¨ Recursive åˆ‡åˆ† ---
        chunks = text_splitter.split_text(content)

        for idx, chunk in enumerate(chunks):
            documents.append(chunk)
            metadatas.append({"source": filename, "type": doc_type, "chunk_index": idx})
            # ID ä¿æŒå”¯ä¸€ï¼Œé¿å…é‡è¤‡å¯«å…¥
            ids.append(f"{filename}_chunk_{idx}")

    if documents:
        cprint(f"ğŸ’¾ æ­£åœ¨å¯«å…¥ {len(documents)} ç­†è³‡æ–™ç‰‡æ®µ...", "yellow")
        try:
            # Upsert: å¦‚æœ ID å­˜åœ¨å°±æ›´æ–°ï¼Œä¸å­˜åœ¨å°±æ–°å¢
            collection.upsert(documents=documents, metadatas=metadatas, ids=ids)
            cprint(f"âœ… è³‡æ–™æ³¨å…¥å®Œæˆï¼è³‡æ–™åº«ç¸½ç­†æ•¸: {collection.count()}", "green")
        except Exception as e:
            cprint(f"âŒ å¯«å…¥å¤±æ•—: {e}", "red")
    else:
        cprint("âš ï¸ ç„¡æœ‰æ•ˆè³‡æ–™ã€‚", "yellow")

if __name__ == "__main__":
    ingest_data()